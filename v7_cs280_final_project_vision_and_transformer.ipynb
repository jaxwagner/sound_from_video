{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1683237452136,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "PKUb0VpR2XPD"
   },
   "outputs": [],
   "source": [
    "#!sudo pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1683237452136,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "JqSUtN2m2Zzs"
   },
   "outputs": [],
   "source": [
    "#from pytube import YouTube\n",
    "def download(url):\n",
    "    youtubeObject = YouTube(url)\n",
    "    youtubeObject = youtubeObject.streams.get_lowest_resolution()\n",
    "    try:\n",
    "        youtubeObject.download()\n",
    "    except:\n",
    "        print(\"An error has occurred\")\n",
    "    print(\"Download is completed successfully\")\n",
    "\n",
    "#url = \"https://www.youtube.com/watch?v=FJZ-BHBKyos\" # car chase\n",
    "#url = \"https://www.youtube.com/watch?v=1gwglom4FeA\" # 8 hr nature\n",
    "#url = \"https://www.youtube.com/watch?v=8W1qF7l2A1c\" # 6 hr nature\n",
    "#download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3543,
     "status": "ok",
     "timestamp": 1683237457375,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "SIxci6qbzEXH",
    "outputId": "5e3d051b-77c6-4eea-fc1a-1edad0abcebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-video in /global/u2/m/martel/.local/perlmutter/pytorch1.13.1/lib/python3.9/site-packages (1.1.11)\n",
      "Requirement already satisfied: pillow in /global/u2/m/martel/.local/perlmutter/pytorch1.13.1/lib/python3.9/site-packages (from scikit-video) (9.5.0)\n",
      "Requirement already satisfied: numpy in /global/u2/m/martel/.local/perlmutter/pytorch1.13.1/lib/python3.9/site-packages (from scikit-video) (1.24.1)\n",
      "Requirement already satisfied: scipy in /global/u2/m/martel/.local/perlmutter/pytorch1.13.1/lib/python3.9/site-packages (from scikit-video) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install scikit-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1683237457788,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "3QAKUr-6AbB7"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import skvideo.io  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 17162,
     "status": "ok",
     "timestamp": 1683237474949,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "MCia8s07Ph_C"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "##NOTE: use vreader and then create a downsampled np array\n",
    "## try 36x64\n",
    "resized_vid_arr = []\n",
    "videodata = skvideo.io.vread(\"car_chase.mp4\")  \n",
    "for v in videodata:\n",
    "  roi = cv2.resize(v, (36, 64))\n",
    "  roi = roi.astype(\"float\") / 255.0\n",
    "  # roi = img_to_array(roi)\n",
    "  # roi = np.expand_dims(roi, axis=0)\n",
    "  resized_vid_arr.append(roi) \n",
    "resized_vid_arr = np.array(resized_vid_arr)\n",
    "'''\n",
    "resized_vid_arr = np.load(\"resized_vid_arr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39977099.83529407"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_vid_arr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18547, 64, 36, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_vid_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2249,
     "status": "ok",
     "timestamp": 1683237477187,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "48DV4rShBh6P",
    "outputId": "71a5d192-0e97-4cfc-9ee4-c67c9f821c19"
   },
   "outputs": [],
   "source": [
    "from moviepy.video.io.VideoFileClip import AudioFileClip\n",
    "audioclip = AudioFileClip('car_chase.mp4', fps=9000)\n",
    "# audioclip = videoclip.audio\n",
    "audio_array = audioclip.to_soundarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1683237477188,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "U5XJo3TcFxpQ"
   },
   "outputs": [],
   "source": [
    "video_and_audio_arr = []\n",
    "aud_per_frame = audio_array.shape[0]// resized_vid_arr.shape[0]\n",
    "for i in range(len(resized_vid_arr)):\n",
    "  aud_in = audio_array[i*aud_per_frame : (i+1)*aud_per_frame+1]\n",
    "  vec_in = [resized_vid_arr[i], aud_in]\n",
    "  video_and_audio_arr.append(vec_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1683237477188,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "Lc0y21sYKck7"
   },
   "outputs": [],
   "source": [
    "fps = 30\n",
    "def make_frame(t):\n",
    "  t = int(t * fps)\n",
    "  # print(t)\n",
    "  return video_and_audio_arr[t][0]\n",
    "\n",
    "from moviepy.video.io.VideoFileClip import VideoClip\n",
    "myclip = VideoClip(make_frame, duration = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1683237477188,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "-iBAZyubIpWH",
    "outputId": "db5114d7-4cc8-49fe-bfef-fc46b45426ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video test.mp4.\n",
      "Moviepy - Writing video test.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "myclip.write_videofile('test.mp4', fps = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1434,
     "status": "ok",
     "timestamp": 1683237478619,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "v6Cf944EXQd_",
    "outputId": "73683f4e-7a36-4056-e2cc-0908fcbeca2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(241101, 2)\n"
     ]
    }
   ],
   "source": [
    "## concatonate the audio\n",
    "audio = video_and_audio_arr[0][1]\n",
    "for i in range(20*40):\n",
    "  next_aud = video_and_audio_arr[i+1][1]\n",
    "  audio = np.vstack((audio, next_aud))\n",
    "print(audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3285,
     "status": "ok",
     "timestamp": 1683237481902,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "OW5Cdk0EZapo",
    "outputId": "788fb668-a99c-4b2a-9092-65270d1e6e8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pydub in /global/u2/m/martel/.local/perlmutter/pytorch1.13.1/lib/python3.9/site-packages (0.25.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1683237481903,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "z1UTKIkuYTMm"
   },
   "outputs": [],
   "source": [
    "import pydub \n",
    "\n",
    "def write(f, sr, x, normalized=True):\n",
    "    \"\"\"numpy array to MP3\"\"\"\n",
    "    channels = 2 if (x.ndim == 2 and x.shape[1] == 2) else 1\n",
    "    if normalized:  # normalized array - each item should be a float in [-1, 1)\n",
    "        y = np.int16(x * 2 ** 15)\n",
    "    else:\n",
    "        y = np.int16(x)\n",
    "    song = pydub.AudioSegment(y.tobytes(), frame_rate=sr, sample_width=2, channels=channels)\n",
    "    song.export(f, format=\"mp3\", bitrate=\"320k\")\n",
    "\n",
    "# sr = (audio_array.shape[0]// resized_vid_arr.shape[0])*fps\n",
    "#write('test_audio_2.mp3', sr, audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q8jot63w2ZZ"
   },
   "source": [
    "*training loop from cs189 hw*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1683237484501,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "MkAHFi0HxVso"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1683237484501,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "E2TzvSjoxnJz"
   },
   "outputs": [],
   "source": [
    "class VideoAudioDataset(Dataset):\n",
    "    def __init__(self, video_frames, audio_frames, num_frames):\n",
    "        self.video_frames = torch.tensor(video_frames, dtype=torch.float32).permute(0,3,2,1) # Permute to (N, C, H, W)\n",
    "        self.aud_per_frame = audio_frames.shape[0]// (video_frames.shape[0])\n",
    "        clip_amount = audio_frames.shape[0] % self.aud_per_frame\n",
    "        self.audio_frames = torch.tensor(audio_frames[:-clip_amount], dtype=torch.float32).reshape(-1, self.aud_per_frame,audio_frames.shape[1]).permute(0,2,1) # Permute to (N, C, A)\n",
    "        print(self.audio_frames.shape)\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += 1\n",
    "        if idx < self.num_frames:\n",
    "          num_zeros_needed = self.num_frames - idx\n",
    "          vid_zeros = torch.zeros(num_zeros_needed, *self.video_frames[0].shape)\n",
    "          aud_zeros = torch.zeros(num_zeros_needed, *self.audio_frames[0].shape)\n",
    "          vid = torch.vstack((vid_zeros, self.video_frames[0:idx])).transpose(0,1)\n",
    "          aud = torch.vstack((aud_zeros, self.audio_frames[0:idx])).transpose(0,1).reshape(2,-1).transpose(0,1)\n",
    "          #print(\"aud_shape:\", aud.shape, \"reshaped:\", torch.vstack((aud_zeros, self.audio_frames[0:idx])).transpose(0,1).shape)\n",
    "          return (vid, aud)\n",
    "        #vid shape example torch.Size([32, 3, 10, 36, 64])\n",
    "        # aud shape example torch.Size([32, 2, 14710])\n",
    "        vid = self.video_frames[idx-self.num_frames:idx].transpose(0,1)\n",
    "        aud = self.audio_frames[idx-self.num_frames: (idx)].transpose(0,1).reshape(2,-1).transpose(0,1)\n",
    "        #print(\"aud_shape:\", aud.shape)\n",
    "        # print('idx = ', idx, ' aud.size = ', aud.shape)\n",
    "        return (vid, aud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBlme3Oac8xI"
   },
   "source": [
    "https://github.com/antecessor/Wavenet << source!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683237484501,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "UfWN3pHBlP5V"
   },
   "outputs": [],
   "source": [
    "class AudConvEmbedding(nn.Module):\n",
    "  def __init__(self, in_channels, output_dim):\n",
    "    super(AudConvEmbedding, self).__init__()\n",
    "    self.conv_layers = nn.Sequential(\n",
    "        nn.Conv1d(in_channels = 2, out_channels = 2, kernel_size = 5, stride = 2),\n",
    "        nn.LayerNorm(7353),\n",
    "        nn.Conv1d(in_channels = 2, out_channels = 2, kernel_size = 5, stride = 2),\n",
    "        nn.LayerNorm(3675),\n",
    "        nn.Conv1d(in_channels = 2, out_channels = 2, kernel_size = 5, stride = 2),\n",
    "        nn.LayerNorm(1836),\n",
    "        nn.Conv1d(in_channels = 2, out_channels = 2, kernel_size = 5, stride = 2),\n",
    "        nn.LayerNorm(916),\n",
    "        nn.Conv1d(in_channels = 2, out_channels = 2, kernel_size = 5, stride = 2),\n",
    "        nn.LayerNorm(456),\n",
    "        nn.Linear(456, output_dim)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.conv_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1683237538333,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "HQcJkvjArLHd"
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def scaled_dot_product(self, q, k, v, mask=None):\n",
    "        d_k = q.size()[-1]\n",
    "        attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "        attn_logits = attn_logits / np.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "        attention = F.softmax(attn_logits, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        #print(q.shape, k.shape, v.shape)\n",
    "        # Determine value outputs\n",
    "        values, attention = self.scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        #print(values.shape)\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1683237538333,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "QYSe-ZnNdr9Y"
   },
   "outputs": [],
   "source": [
    "class AddPosEncoding(nn.Module):\n",
    "  def __init__(self, d_model = 256, input_dropout = 0.1, timing_dropout = 0.1, max_len = 512):\n",
    "    super(AddPosEncoding, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.input_dropout = input_dropout\n",
    "    self.timing_dropout = timing_dropout\n",
    "    self.max_len = max_len\n",
    "\n",
    "    self.timing_table = nn.Parameter(torch.FloatTensor(max_len, d_model))\n",
    "    nn.init.normal_(self.timing_table)\n",
    "    self.input_dropout = nn.Dropout(input_dropout)\n",
    "    self.timing_dropout = nn.Dropout(self.timing_dropout)\n",
    "\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = self.input_dropout(x)\n",
    "    timing = self.timing_table[None, :x.shape[1], :]\n",
    "    timing = self.timing_dropout(timing)\n",
    "    return x + timing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUTanh(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(GLUTanh, self).__init__()\n",
    "        self.glu = nn.GLU(dim=-1)\n",
    "        self.linear = nn.Linear(input_size // 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.glu(x)\n",
    "        x = self.linear(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1683237538935,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "0tYyj_yXc1mE"
   },
   "outputs": [],
   "source": [
    "class AudTransformer(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, hidden_dims = 256, seq_len = 100, dim_ff = 1024, n_layers = 6, n_head = 8, d_qkv = 64, dropout = 0.1):\n",
    "        super(AudTransformer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.dim_ff = dim_ff\n",
    "        self.n_layers = n_layers\n",
    "        self.n_head = n_head\n",
    "        self.d_qkv = d_qkv\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.mha_list = nn.ModuleList()\n",
    "        self.mha_norms = nn.ModuleList()\n",
    "        self.pff_list = nn.ModuleList()\n",
    "        self.pff_norms = nn.ModuleList()\n",
    "\n",
    "        #self.output_norm = nn.LayerNorm(hidden_dims)\n",
    "        #self.linear = nn.Linear(self.hidden_dims, 2)\n",
    "        #self.tanh = nn.Tanh()\n",
    "\n",
    "        self.add_timing = AddPosEncoding(hidden_dims, max_len = seq_len)\n",
    "\n",
    "        #self.embedding = AudConvEmbedding(2, self.hidden_dims)\n",
    "        self.embedding2 = nn.Linear(2, self.hidden_dims)\n",
    "        #self.output_proj = nn.Conv1d(in_channels = seq_len, out_channels = 1, kernel_size = 1, stride = 1)\n",
    "\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dims),\n",
    "            nn.Conv1d(in_channels = seq_len, out_channels = 1, kernel_size = 1, stride = 1),\n",
    "            nn.Linear(self.hidden_dims, 1024),\n",
    "            nn.GLU(),\n",
    "            nn.Linear(512, 4)\n",
    "        )\n",
    "        \n",
    "\n",
    "        for _ in range(n_layers):\n",
    "          self.mha_list.append(MultiheadAttention(self.hidden_dims, self.d_qkv, self.n_head, self.dropout))\n",
    "          self.mha_norms.append(nn.LayerNorm(hidden_dims))\n",
    "          self.pff_list.append(nn.Sequential(\n",
    "                              nn.Linear(hidden_dims, dim_ff),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(dim_ff, hidden_dims),\n",
    "                              nn.Dropout(self.dropout)\n",
    "                              ))\n",
    "          self.pff_norms.append(nn.LayerNorm(hidden_dims))\n",
    "\n",
    "  def forward(self, x):\n",
    "    #print(\"tf:\", x.shape)\n",
    "    #x = self.embedding(x).transpose(2,1)\n",
    "    x = self.embedding2(x)\n",
    "    x = x + self.add_timing(x)\n",
    "    #print(\"tf emb:\", x.shape)\n",
    "    for i in range(self.n_layers):\n",
    "      x_norm = self.mha_norms[i](x)\n",
    "      #print('x_norm.shape', x_norm.shape)\n",
    "      att_out = self.mha_list[i](x)\n",
    "      #print(att_out.shape)\n",
    "      x = x_norm + att_out\n",
    "      x_norm = self.pff_norms[i](x)\n",
    "      ff_out = self.pff_list[i](x_norm)\n",
    "      x = x_norm + ff_out\n",
    "      #print(\"tf layer:\", i, x.shape)\n",
    "    \n",
    "    #x = self.output_norm(x)\n",
    "    #print(\"tf:\", x.shape)\n",
    "    #x = self.linear(x)\n",
    "    x = self.output_proj(x)\n",
    "    #print(\"tf:\", x.shape)\n",
    "    #print(\"tf:\", x.shape)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683237539153,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "uHtt4x660lxE"
   },
   "outputs": [],
   "source": [
    "class VidToAudFusion(nn.Module):\n",
    "    def __init__(self, audio_dim, video_dims, audio_channels, vid_channels, dropout_prob=0.1, num_frames=10):\n",
    "        super(VidToAudFusion, self).__init__()\n",
    "        self.dim_proj1 = nn.Linear(video_dims[0] * video_dims[1], 200)\n",
    "        self.dim_proj2 = nn.Linear(200 * num_frames, (audio_dim) - 1)\n",
    "        self.audio_dim = audio_dim\n",
    "        self.video_dims = video_dims\n",
    "        self.vid_channels = vid_channels\n",
    "\n",
    "        if audio_channels == vid_channels:\n",
    "            self.channel_projection = nn.Identity()\n",
    "        else:\n",
    "            self.channel_projection = nn.Sequential(\n",
    "                nn.Conv2d(vid_channels, audio_channels, kernel_size=1, stride=1, bias=True),\n",
    "            )\n",
    "        self.channel_projection2 = nn.Sequential(\n",
    "                nn.Conv1d(num_frames, 2, kernel_size=1, stride=1, bias=True),\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "      #x = x.reshape(x.shape[0], x.shape[1], x.shape[2], -1)\n",
    "      #print(\"vid to aud:\", x.shape)\n",
    "      audio_proj = torch.flatten(x, start_dim=3)\n",
    "      #print(\"vid to aud:\", x.shape)\n",
    "      #print(audio_proj.shape)  \n",
    "      audio_proj = self.dim_proj1(audio_proj)\n",
    "      #print(\"vid to aud:\", audio_proj.shape)\n",
    "      #print(audio_proj.shape)\n",
    "      audio_proj = self.channel_projection(audio_proj).squeeze(2)\n",
    "      #print(\"vid to aud:\", audio_proj.shape)\n",
    "      #audio_proj = self.channel_projection2(audio_proj)\n",
    "      audio_proj = audio_proj.reshape(audio_proj.shape[0], -1)\n",
    "      #print(\"vid to aud:\", audio_proj.shape)\n",
    "      audio_proj = self.dim_proj2(audio_proj)\n",
    "      #print(\"vid to aud:\", audio_proj.shape)\n",
    "      #print(audio_proj.shape)\n",
    "      return audio_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683237539153,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "zXf3OZlZyA8M"
   },
   "outputs": [],
   "source": [
    "class ConvResBlock3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.1, stride=1, kernel_size=3, padding=1):\n",
    "        super(ConvResBlock3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "        self.norm1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=True)\n",
    "        self.norm2 = nn.BatchNorm3d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        if in_channels == out_channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        else:\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride, bias=True),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        residual = self.skip_connection(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        out = out + residual\n",
    "\n",
    "        return out\n",
    "\n",
    "class ConvResBlock2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.1, stride=1, kernel_size=3, padding=1):\n",
    "        super(ConvResBlock2D, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=True)\n",
    "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        if in_channels == out_channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        else:\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=True),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        residual = self.skip_connection(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        out = out + residual\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683237539153,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "Ue7zYZltyIe4"
   },
   "outputs": [],
   "source": [
    "### combines video into audio input vector and then passes through wavenet\n",
    "\n",
    "class AttAudVideoNet(nn.Module):\n",
    "    def __init__(self,audio_input_shape, video_input_shape,in_channels=2,out_channels=2, seq_len=1470, num_frames=5): ## NEED TO DEBUG THESE hyperperams 4.29\n",
    "        super().__init__()\n",
    "        self.transformer=AudTransformer(in_channels, out_channels, hidden_dims = 256, seq_len = seq_len, dim_ff = 2048, n_layers = 8, n_head = 6, d_qkv = 60, dropout = 0.1) \n",
    "        self.activation = GLUTanh(4,2)\n",
    "        self.vid_convs = nn.ModuleList([\n",
    "            ConvResBlock3D(3, 128),\n",
    "            ConvResBlock3D(128, 256),\n",
    "            ConvResBlock3D(256, 64),\n",
    "            ConvResBlock3D(64, 8)\n",
    "        ])\n",
    "        self.audio_input_shape = audio_input_shape\n",
    "        self.video_input_shape = video_input_shape\n",
    "        vid_dims_list = self.get_video_dims(video_input_shape)\n",
    "        self.vid_to_aud = VidToAudFusion(self.audio_input_shape[-1], vid_dims_list[2], 1, 8, num_frames=num_frames)\n",
    "        \n",
    "        #self.output_lin = nn.Linear(256*in_channels, out_channels)\n",
    "\n",
    "    def get_video_dims(self, video_input_shape):\n",
    "        shape_list = []\n",
    "        test_data = torch.ones(video_input_shape)\n",
    "        out = self.vid_convs[0](test_data)\n",
    "        shape_list.append((out.shape[-2],out.shape[-1]))\n",
    "        for layer in self.vid_convs[1:]:\n",
    "            out = layer(out)\n",
    "            shape_list.append((out.shape[-2],out.shape[-1]))\n",
    "        return shape_list\n",
    "\n",
    "    def forward(self,vid, aud):\n",
    "        #print(vid.shape, aud.shape)\n",
    "        for i in range(len(self.vid_convs)):\n",
    "          vid = self.vid_convs[i](vid)\n",
    "        #print(vid.shape)\n",
    "        vid_to_aud = self.vid_to_aud(vid)\n",
    "        #print(vid_to_aud.shape, aud.shape)\n",
    "        #print(vid_to_aud.shape, aud.shape)\n",
    "        aud = aud + vid_to_aud.unsqueeze(2)\n",
    "        aud=self.transformer(aud)\n",
    "        #print(aud.shape)\n",
    "        #aud=aud.reshape(aud.shape[0], -1)\n",
    "        #aud = self.output_lin(aud)\n",
    "        #print('output shape = ', aud.shape)\n",
    "        return self.activation(aud)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683237539153,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "N8JUAOeGGFnG",
    "outputId": "28b9d3b9-3c55-43f1-b32a-85f540449425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18547, 64, 36, 3) 128196864\n",
      "11139480\n"
     ]
    }
   ],
   "source": [
    "print(resized_vid_arr.shape, resized_vid_arr.size)\n",
    "print(audio_array.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1683237539810,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "Bs_v_OsGl5gC",
    "outputId": "29520eaa-b87e-4f16-bffd-ca0d3ce666f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14852, 2, 300])\n",
      "torch.Size([3713, 2, 300])\n"
     ]
    }
   ],
   "source": [
    "num_frames = 5\n",
    "train_data = VideoAudioDataset(resized_vid_arr[:int(len(resized_vid_arr)*0.8)], audio_array[:int(len(audio_array)*0.8)], num_frames = num_frames)\n",
    "valid_data = VideoAudioDataset(resized_vid_arr[int(len(resized_vid_arr)*0.8):], audio_array[int(len(audio_array)*0.8):], num_frames = num_frames)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1683237541414,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "3flVTs2Q0n2e",
    "outputId": "74a7d44f-010c-4644-b20f-ff5fdbcc8fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 5, 36, 64])\n",
      "torch.Size([16, 1500, 2])\n"
     ]
    }
   ],
   "source": [
    "for x in train_loader:\n",
    "  print(x[0].shape)\n",
    "  print(x[1].shape)\n",
    "  break\n",
    "  \n",
    "count = 0\n",
    "# for x in valid_loader:\n",
    "#   count += 1\n",
    "#   if count == 10:\n",
    "#     print(x[0].shape)\n",
    "#     print(x[1].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "73273624d3ec46539047695496cef11b",
      "bbada1e125ff429eae418bf609e3ea4c",
      "9f611faec9f24d7fb136eae0dcaf71cf",
      "9c643f04c1b741d1b57fa7b6b209c102",
      "fdbb2926241c4891885ef8a36ce7a130",
      "46105105d11e435dbc0db488c43fdf47",
      "675868afca70457aacba30a626a61b35",
      "cdfb1461201943b183377eefdb6772d6",
      "3b20e87dd300401b82d747d27ecbc039",
      "d226292202f1430cbdc602085458ad03",
      "acadf0fed9184df98a87485271518fdc",
      "bda87b78f0cd420bb99623ea2fa2759f",
      "66028dc242a24a6bbe5aae7d5aac4ee0",
      "b23a60ed465e45beb5cd4bad947d1270",
      "1599d35bd6844badaeedd1fef5d86877",
      "85c3ed4a97f2464e834d340cf7d43006",
      "a646b672a12442fab793b40b5f019547",
      "fad159c751df456285bc240fd603ff37",
      "919ddb8d374b4e92b57f750c0d019859",
      "1bf569533ba648e08eedf0b0a2a9cdf9",
      "347b450562614c51b1295c8bddd69ade",
      "2f60c766deac41baa3e771dc24800dd1"
     ]
    },
    "id": "cz9cYVhaa0vj",
    "outputId": "e081301b-d979-4824-de7a-0b4008a8aa27"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcff14aea14f480d9556a817a61ae490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a8c64ac97a48e1acfa66e6388cf8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0836, -0.2140],\n",
      "        [-0.3243, -0.8008],\n",
      "        [-0.2607, -0.6306]], device='cuda:0')\n",
      "tensor([[-0.0332,  0.5647],\n",
      "        [-0.0334,  0.5652],\n",
      "        [-0.0328,  0.5682]], device='cuda:0')\n",
      "loss for step 0 : 0.00533019143116215\n",
      "epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceda5af5093641a2a4fc96acb6fb9d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e4da1a64734df9a20bbebecce2c5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0416, -0.2072],\n",
      "        [ 0.3305,  0.5553],\n",
      "        [ 0.3204,  0.0612]], device='cuda:0')\n",
      "tensor([[-0.0362,  0.5600],\n",
      "        [-0.0090,  0.6139],\n",
      "        [-0.0077,  0.6150]], device='cuda:0')\n",
      "loss for step 0 : 0.005243558922539587\n",
      "epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c079478a543341e3816860be45487be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fd27c41cc64ffe9ef0a5cd08b2cd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2596,  0.5251],\n",
      "        [ 0.3622,  0.1263],\n",
      "        [-0.2284, -0.4983]], device='cuda:0')\n",
      "tensor([[-0.0172,  0.5921],\n",
      "        [-0.0235,  0.5819],\n",
      "        [-0.0282,  0.5754]], device='cuda:0')\n",
      "loss for step 0 : 0.005344857364569021\n",
      "epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916749aa0270495f912b0aaf255dee16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f19d82024f645a8bd94eebf9659e450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4168, -0.3827],\n",
      "        [-0.4896, -0.7022],\n",
      "        [ 0.2686,  0.2097]], device='cuda:0')\n",
      "tensor([[0.0367, 0.6463],\n",
      "        [0.0298, 0.6442],\n",
      "        [0.0306, 0.6440]], device='cuda:0')\n",
      "loss for step 0 : 0.005428738875881485\n",
      "epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88be8d1a9f0d44eea3a9d1ac2aaf0a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c715cc744004a928f20593335ca890f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0954, -0.0420],\n",
      "        [-0.0052, -0.0504],\n",
      "        [ 0.3346,  0.2849]], device='cuda:0')\n",
      "tensor([[0.0171, 0.6193],\n",
      "        [0.0202, 0.6224],\n",
      "        [0.0204, 0.6224]], device='cuda:0')\n",
      "loss for step 0 : 0.005378820726890927\n",
      "epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906a172ce46e49a8a8a92c514516ed9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6605532590bc48c89676d02603012e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2821, -0.3802],\n",
      "        [ 0.1873,  0.1178],\n",
      "        [ 0.0103,  0.4026]], device='cuda:0')\n",
      "tensor([[0.0068, 0.6077],\n",
      "        [0.0035, 0.6044],\n",
      "        [0.0059, 0.6070]], device='cuda:0')\n",
      "loss for step 0 : 0.005369510679789211\n",
      "epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf5b551725d454baaafa4ebe90855db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ee386668ac482cb7f373f840f2143d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0668,  0.0955],\n",
      "        [-0.0309,  0.0524],\n",
      "        [-0.3716, -0.2607]], device='cuda:0')\n",
      "tensor([[-9.7192e-04,  5.9859e-01],\n",
      "        [ 1.3735e-04,  5.9972e-01],\n",
      "        [ 5.5144e-04,  6.0013e-01]], device='cuda:0')\n",
      "loss for step 0 : 0.005359386516046589\n",
      "epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd8badabbfb4acdb9e1a716bdc8a086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7efcd422dd4eec9ea6430b29f0cfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3311,  0.0676],\n",
      "        [-0.3234, -0.4106],\n",
      "        [-0.1933, -0.1794]], device='cuda:0')\n",
      "tensor([[-0.0056,  0.5940],\n",
      "        [-0.0052,  0.5944],\n",
      "        [-0.0045,  0.5952]], device='cuda:0')\n",
      "loss for step 0 : 0.005305743804606407\n",
      "epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1771118841584bb8a087ff40f4c9c729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b5b9498b534b70aadfacc93b7dd3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0262, -0.2697],\n",
      "        [-0.1903,  0.0046],\n",
      "        [ 0.1716,  0.3402]], device='cuda:0')\n",
      "tensor([[-1.3402e-03,  5.9857e-01],\n",
      "        [-1.9684e-03,  5.9795e-01],\n",
      "        [-5.7463e-04,  5.9935e-01]], device='cuda:0')\n",
      "loss for step 0 : 0.005325451059995786\n",
      "epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a3e2ccfe104adbb51335d800d09114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e6ab08b8fc4568a59935126544606b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1607,  0.0799],\n",
      "        [ 0.1143,  0.1971],\n",
      "        [ 0.1465,  0.2224]], device='cuda:0')\n",
      "tensor([[-0.0196,  0.5804],\n",
      "        [-0.0195,  0.5805],\n",
      "        [-0.0193,  0.5806]], device='cuda:0')\n",
      "loss for step 0 : 0.005393363470616548\n",
      "epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e8c892058445128ea640404e8e38e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff75b1085594a8caefbc35b29a40b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0156,  0.1667],\n",
      "        [-0.1223, -0.1744],\n",
      "        [ 0.0780,  0.0391]], device='cuda:0')\n",
      "tensor([[-0.0329,  0.5667],\n",
      "        [-0.0328,  0.5667],\n",
      "        [-0.0328,  0.5668]], device='cuda:0')\n",
      "loss for step 0 : 0.005383393990442804\n",
      "epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f731691b4348c3bca00e022454ee8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1127f4e2444fba8169767ca67b68bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6973, -0.2820],\n",
      "        [ 0.3960,  0.5219],\n",
      "        [ 0.2460,  0.2317]], device='cuda:0')\n",
      "tensor([[-0.0336,  0.5660],\n",
      "        [-0.0335,  0.5660],\n",
      "        [-0.0337,  0.5659]], device='cuda:0')\n",
      "loss for step 0 : 0.005412422987106054\n",
      "epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205b2a7615e240fea55f288e33f947e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733d5999aa3141a885bb7a7bf46365a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4523,  0.5114],\n",
      "        [ 0.0421, -0.1011],\n",
      "        [-0.5169,  0.2387]], device='cuda:0')\n",
      "tensor([[-0.0352,  0.5648],\n",
      "        [-0.0352,  0.5647],\n",
      "        [-0.0352,  0.5648]], device='cuda:0')\n",
      "loss for step 0 : 0.005413105650602475\n",
      "epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d011d03de66549538b1f6c5d063f5b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c71c7e433024d28b8fd828a2a7479ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0791, -0.4767],\n",
      "        [ 0.7174,  0.4148],\n",
      "        [-0.1507,  0.1315]], device='cuda:0')\n",
      "tensor([[-0.0355,  0.5646],\n",
      "        [-0.0355,  0.5646],\n",
      "        [-0.0355,  0.5646]], device='cuda:0')\n",
      "loss for step 0 : 0.005374142151002003\n",
      "epoch 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979e6ff7d03f4fb6a222477b57ffbab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0986085fa5074f3c86fe980cb4005180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0723, -0.0804],\n",
      "        [-0.3672, -0.2056],\n",
      "        [ 0.2580, -0.1913]], device='cuda:0')\n",
      "tensor([[-0.0358,  0.5645],\n",
      "        [-0.0358,  0.5645],\n",
      "        [-0.0358,  0.5645]], device='cuda:0')\n",
      "loss for step 0 : 0.0053249548148849735\n",
      "epoch 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273e454e215b460ca765122bcc37e6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268532e78a72439ab8ef8b5cb66eeec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4247,  0.2318],\n",
      "        [ 0.0030,  0.2096],\n",
      "        [ 0.3585,  0.1858]], device='cuda:0')\n",
      "tensor([[-0.0361,  0.5645],\n",
      "        [-0.0361,  0.5645],\n",
      "        [-0.0361,  0.5645]], device='cuda:0')\n",
      "loss for step 0 : 0.005392076431409172\n",
      "epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082113581d8540d492c048ec3364208b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904fbe2039db4d70b2db1801956cedb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1944,  0.2551],\n",
      "        [-0.0170, -0.0097],\n",
      "        [-0.0659, -0.2724]], device='cuda:0')\n",
      "tensor([[-0.0364,  0.5645],\n",
      "        [-0.0364,  0.5645],\n",
      "        [-0.0364,  0.5645]], device='cuda:0')\n",
      "loss for step 0 : 0.005295283080119154\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     51\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 52\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m%\u001b[39mglobalStep\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     55\u001b[0m    \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[1;32m     56\u001b[0m    \u001b[38;5;66;03m# print(output.detach().numpy())\u001b[39;00m\n\u001b[1;32m     57\u001b[0m    \u001b[38;5;66;03m# print(y_train.numpy())\u001b[39;00m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/optim/adamw.py:316\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    314\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    318\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# wavenet = WaveNet(in_channels=2,out_channels=2,kernel_size=2,stack_size=23,layer_size=6).cuda().train()\n",
    "#audio_test_data = torch.ones((1,2,1471))\n",
    "seq_len = 300 * num_frames\n",
    "audio_test_data = torch.ones((1,2,seq_len))\n",
    "vid_test_data = torch.ones((1,3,num_frames,36,64))\n",
    "\n",
    "wavenet = AttAudVideoNet(audio_input_shape=audio_test_data.shape, video_input_shape=vid_test_data.shape,in_channels=2,out_channels=2, seq_len=seq_len-1, num_frames=num_frames).cuda().train()\n",
    "#load_path = 'transformer_vid_model3_5.7.pt'\n",
    "#wavenet.load_state_dict(torch.load(load_path))\n",
    "\n",
    "lr = 4e-5\n",
    "epochs= 50\n",
    "globalStep=1000\n",
    "\n",
    "optimizer=torch.optim.AdamW(wavenet.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "  optimizer,\n",
    "  lr,\n",
    "  epochs=epochs,\n",
    "  steps_per_epoch=len(train_loader),\n",
    "  pct_start=0.03,  # Warm up for 3% of the total training time\n",
    "  )\n",
    "lossFunction = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def calc_accuracy(Out,Y):\n",
    "    max_vals, max_indices = torch.max(Out,1)\n",
    "    train_acc = (max_indices == Y).sum().item()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "  \n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, (vid_frames,aud_frames) in tqdm(enumerate(train_loader),desc=\"Training\"):\n",
    "         #vid_frames = vid_frames.cuda()\n",
    "         target = aud_frames[:,-1,].cuda()\n",
    "         aud_frames = aud_frames[:,:-1,:].cuda()\n",
    "         vid_frames = vid_frames.cuda()\n",
    "         #print(target.shape)\n",
    "         #print(aud_frames.shape)\n",
    "         output = wavenet(vid_frames, aud_frames).squeeze()\n",
    "         #print('ttt', output.shape, target.shape)\n",
    "         #print(output[0].detach().cpu().numpy(), target[0].cpu().numpy())\n",
    "         #print(output.shape)\n",
    "         #print(output)\n",
    "         #print(output.dtype, target.dtype)\n",
    "\n",
    "         loss = lossFunction(output,target)\n",
    "         optimizer.zero_grad()\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         scheduler.step()\n",
    "         if step%globalStep==0:\n",
    "            # scheduler.step()\n",
    "            # print(output.detach().numpy())\n",
    "            # print(y_train.numpy())\n",
    "            with torch.no_grad():\n",
    "                accuracy=0\n",
    "                val_loss=0\n",
    "                for stepTest, (vid_frames,aud_frames) in tqdm(enumerate(valid_loader),desc=\"Validation\"):\n",
    "                    vid_frames = vid_frames.cuda()\n",
    "                    target = aud_frames[:,-1,].cuda()\n",
    "                    aud_frames = aud_frames[:,:-1,:].cuda()\n",
    "                    output = wavenet(vid_frames, aud_frames).squeeze()\n",
    "                    if stepTest==0:\n",
    "                        print(target[:3])\n",
    "                        print(output[:3])\n",
    "                        #print(lossFunction(output[0],target[0]).item())\n",
    "                    #accuracy+=calc_accuracy(output,target)*100\n",
    "                    val_loss+= lossFunction(output,target).item()\n",
    "                    if stepTest>200:\n",
    "                        print(output)\n",
    "                        break\n",
    "            print(f\"loss for step {step} : {val_loss/stepTest}\")\n",
    "\n",
    "         \n",
    "    print(f\"epoch {epoch}\")\n",
    "\n",
    "    save_path = 'transformer_vid_model3_5.10.pt'\n",
    "    torch.save(wavenet.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 268,
     "status": "aborted",
     "timestamp": 1683237494989,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "JvGcaaiR_1wg"
   },
   "outputs": [],
   "source": [
    "#save_path = 'transformer_vid_model2_5.7.pt'\n",
    "#torch.save(wavenet.state_dict(), save_path)\n",
    "load_path = 'transformer_vid_model3_5.10.pt'\n",
    "wavenet.load_state_dict(torch.load(load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1683237494989,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "AV772MGPyhZa"
   },
   "outputs": [],
   "source": [
    "class VideoOnlyDataset(Dataset):\n",
    "    def __init__(self, video_frames, num_frames):\n",
    "        self.video_frames = torch.tensor(video_frames, dtype=torch.float32).permute(0,3,2,1) # Permute to (N, C, H, W)\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += 1\n",
    "        if idx < self.num_frames:\n",
    "          num_zeros_needed = self.num_frames - idx\n",
    "          vid_zeros = torch.zeros(num_zeros_needed, *self.video_frames[0].shape)\n",
    "          vid = torch.vstack((vid_zeros, self.video_frames[0:idx])).transpose(0,1)\n",
    "          return vid\n",
    "        #vid shape example torch.Size([32, 3, 10, 36, 64])\n",
    "        # aud shape example torch.Size([32, 2, 14710])\n",
    "        vid = self.video_frames[idx-self.num_frames:idx].transpose(0,1)\n",
    "        # print('idx = ', idx, ' aud.size = ', aud.shape)\n",
    "        return vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "hTH9qnTGtkYk"
   },
   "outputs": [],
   "source": [
    "### generate audio for video from model\n",
    "num_frames = 1\n",
    "\n",
    "start_index = int(len(resized_vid_arr)*0.8) + num_frames\n",
    "audio_list = []\n",
    "vid_test_shape = (1,3,10,36,64)\n",
    "zero_frame = torch.zeros((vid_test_shape[3], vid_test_shape[4]))\n",
    "\n",
    "aud_per_vid_frame = 1500\n",
    "audio_start_index = int(len(audio_array)*0.8)\n",
    "print(audio_start_index)\n",
    "aud_input_arr = torch.tensor(audio_array[audio_start_index:audio_start_index+(num_frames * aud_per_vid_frame) - 1], dtype=torch.float32).cuda().unsqueeze(0)\n",
    "print(aud_input_arr.shape)\n",
    "\n",
    "#aud_input_arr = torch.zeros((1,2,(num_frames * aud_per_vid_frame) - 1)).cuda()\n",
    "print(aud_input_arr.shape)\n",
    "\n",
    "input_vid = resized_vid_arr[start_index:]\n",
    "gen_data = VideoOnlyDataset(input_vid, num_frames = num_frames)\n",
    "gen_loader = DataLoader(gen_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "#NOTE: GEN NEEDS TO BE FIXED TO ACCOUNT FOR MULTIPLE AUDIO RUNS PER FRAME\n",
    "wavenet.eval()\n",
    "for i,vid in enumerate(gen_loader):\n",
    "    vid = vid.cuda()\n",
    "    print(vid.shape)\n",
    "    for j in trange(aud_per_vid_frame):\n",
    "        with torch.no_grad():\n",
    "            #print(aud_input_arr.shape)\n",
    "            audio_output = wavenet(vid.cuda(), aud_input_arr.cuda()).cpu()\n",
    "            #print(audio_output, audio_output.shape)\n",
    "            audio_list.append(audio_output.squeeze().cpu().numpy())\n",
    "            #print(aud_input_arr)\n",
    "            aud_input_arr = aud_input_arr[:, 1:, :]\n",
    "            #print(aud_input_arr.shape, audio_output.shape)\n",
    "            #print(aud_input_arr)\n",
    "            \n",
    "            aud_input_arr = torch.cat((aud_input_arr, audio_output.cuda()), 1)\n",
    "            #print(aud_input_arr)\n",
    "    if i > 15:\n",
    "        break\n",
    "np.array(audio_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "MOPq8NXlCUlm"
   },
   "outputs": [],
   "source": [
    "print(len(audio_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "gnU6a3aV5Pa4"
   },
   "outputs": [],
   "source": [
    "print(np.array(audio_list).shape)\n",
    "audio_np = np.array(audio_list).transpose(1,0)\n",
    "audio_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "dHmMWv_fVdSp"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "audio_cpu = audio_output.cpu()\n",
    "audio_np = audio_cpu.detach().numpy()\n",
    "audio_np.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test_transformer_model3_out_5.10_1.raw'\n",
    "np.save(path, audio_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "3zx_uQAZPKek"
   },
   "outputs": [],
   "source": [
    "fps = 30\n",
    "sr = (audio_array.shape[0]// resized_vid_arr.shape[0])*fps\n",
    "write('test_transformer_model3_out_5.10_1.mp3', sr, audio_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494991,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "f62amSXe_6Ur"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1hLH14IPIjmhukYwuIVkWH0KAWopjnaIe",
     "timestamp": 1683231309344
    },
    {
     "file_id": "1--9ZUZLPg4PBYeBwTwaaUQqzzREoTy5p",
     "timestamp": 1682834828374
    },
    {
     "file_id": "1DzDk2SAXwqU8pvN5-_zvgopIuH-s9Y0R",
     "timestamp": 1682711665945
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "pytorch-1.13.1",
   "language": "python",
   "name": "pytorch-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1599d35bd6844badaeedd1fef5d86877": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_347b450562614c51b1295c8bddd69ade",
      "placeholder": "",
      "style": "IPY_MODEL_2f60c766deac41baa3e771dc24800dd1",
      "value": " 46/? [00:19&lt;00:00,  2.54it/s]"
     }
    },
    "1bf569533ba648e08eedf0b0a2a9cdf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2f60c766deac41baa3e771dc24800dd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "347b450562614c51b1295c8bddd69ade": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b20e87dd300401b82d747d27ecbc039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "46105105d11e435dbc0db488c43fdf47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66028dc242a24a6bbe5aae7d5aac4ee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a646b672a12442fab793b40b5f019547",
      "placeholder": "",
      "style": "IPY_MODEL_fad159c751df456285bc240fd603ff37",
      "value": "Validation: "
     }
    },
    "675868afca70457aacba30a626a61b35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73273624d3ec46539047695496cef11b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbada1e125ff429eae418bf609e3ea4c",
       "IPY_MODEL_9f611faec9f24d7fb136eae0dcaf71cf",
       "IPY_MODEL_9c643f04c1b741d1b57fa7b6b209c102"
      ],
      "layout": "IPY_MODEL_fdbb2926241c4891885ef8a36ce7a130"
     }
    },
    "85c3ed4a97f2464e834d340cf7d43006": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "919ddb8d374b4e92b57f750c0d019859": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9c643f04c1b741d1b57fa7b6b209c102": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d226292202f1430cbdc602085458ad03",
      "placeholder": "",
      "style": "IPY_MODEL_acadf0fed9184df98a87485271518fdc",
      "value": " 0/? [00:00&lt;?, ?it/s]"
     }
    },
    "9f611faec9f24d7fb136eae0dcaf71cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdfb1461201943b183377eefdb6772d6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3b20e87dd300401b82d747d27ecbc039",
      "value": 0
     }
    },
    "a646b672a12442fab793b40b5f019547": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acadf0fed9184df98a87485271518fdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b23a60ed465e45beb5cd4bad947d1270": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_919ddb8d374b4e92b57f750c0d019859",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1bf569533ba648e08eedf0b0a2a9cdf9",
      "value": 1
     }
    },
    "bbada1e125ff429eae418bf609e3ea4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46105105d11e435dbc0db488c43fdf47",
      "placeholder": "",
      "style": "IPY_MODEL_675868afca70457aacba30a626a61b35",
      "value": "Training: "
     }
    },
    "bda87b78f0cd420bb99623ea2fa2759f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_66028dc242a24a6bbe5aae7d5aac4ee0",
       "IPY_MODEL_b23a60ed465e45beb5cd4bad947d1270",
       "IPY_MODEL_1599d35bd6844badaeedd1fef5d86877"
      ],
      "layout": "IPY_MODEL_85c3ed4a97f2464e834d340cf7d43006"
     }
    },
    "cdfb1461201943b183377eefdb6772d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "d226292202f1430cbdc602085458ad03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fad159c751df456285bc240fd603ff37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fdbb2926241c4891885ef8a36ce7a130": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
