{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1683237452136,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "PKUb0VpR2XPD"
   },
   "outputs": [],
   "source": [
    "#!sudo pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1683237452136,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "JqSUtN2m2Zzs"
   },
   "outputs": [],
   "source": [
    "#from pytube import YouTube\n",
    "def download(url):\n",
    "    youtubeObject = YouTube(url)\n",
    "    youtubeObject = youtubeObject.streams.get_lowest_resolution()\n",
    "    try:\n",
    "        youtubeObject.download()\n",
    "    except:\n",
    "        print(\"An error has occurred\")\n",
    "    print(\"Download is completed successfully\")\n",
    "\n",
    "#url = \"https://www.youtube.com/watch?v=FJZ-BHBKyos\" # car chase\n",
    "#url = \"https://www.youtube.com/watch?v=1gwglom4FeA\" # 8 hr nature\n",
    "#url = \"https://www.youtube.com/watch?v=8W1qF7l2A1c\" # 6 hr nature\n",
    "#download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3543,
     "status": "ok",
     "timestamp": 1683237457375,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "SIxci6qbzEXH",
    "outputId": "5e3d051b-77c6-4eea-fc1a-1edad0abcebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-video in /global/u2/m/martel/.local/perlmutter/pytorch1.13.1/lib/python3.9/site-packages (1.1.11)\n",
      "Requirement already satisfied: scipy in /global/u2/m/martel/.local/perlmutter/pytorch1.13.1/lib/python3.9/site-packages (from scikit-video) (1.10.1)\n",
      "Requirement already satisfied: numpy in /global/u2/m/martel/.local/perlmutter/pytorch1.13.1/lib/python3.9/site-packages (from scikit-video) (1.24.1)\n",
      "Requirement already satisfied: pillow in /global/u2/m/martel/.local/perlmutter/pytorch1.13.1/lib/python3.9/site-packages (from scikit-video) (9.5.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install scikit-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1683237457788,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "3QAKUr-6AbB7"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import skvideo.io  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 17162,
     "status": "ok",
     "timestamp": 1683237474949,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "MCia8s07Ph_C"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "##NOTE: use vreader and then create a downsampled np array\n",
    "## try 36x64\n",
    "resized_vid_arr = []\n",
    "videodata = skvideo.io.vread(\"car_chase.mp4\")  \n",
    "for v in videodata:\n",
    "  roi = cv2.resize(v, (36, 64))\n",
    "  roi = roi.astype(\"float\") / 255.0\n",
    "  # roi = img_to_array(roi)\n",
    "  # roi = np.expand_dims(roi, axis=0)\n",
    "  resized_vid_arr.append(roi) \n",
    "resized_vid_arr = np.array(resized_vid_arr)\n",
    "'''\n",
    "resized_vid_arr = np.load(\"resized_vid_arr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39977099.83529407"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_vid_arr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18547, 64, 36, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_vid_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2249,
     "status": "ok",
     "timestamp": 1683237477187,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "48DV4rShBh6P",
    "outputId": "71a5d192-0e97-4cfc-9ee4-c67c9f821c19"
   },
   "outputs": [],
   "source": [
    "from moviepy.video.io.VideoFileClip import AudioFileClip\n",
    "audioclip = AudioFileClip('car_chase.mp4')\n",
    "# audioclip = videoclip.audio\n",
    "audio_array = audioclip.to_soundarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1683237477188,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "U5XJo3TcFxpQ"
   },
   "outputs": [],
   "source": [
    "video_and_audio_arr = []\n",
    "aud_per_frame = audio_array.shape[0]// resized_vid_arr.shape[0]\n",
    "for i in range(len(resized_vid_arr)):\n",
    "  aud_in = audio_array[i*aud_per_frame : (i+1)*aud_per_frame+1]\n",
    "  vec_in = [resized_vid_arr[i], aud_in]\n",
    "  video_and_audio_arr.append(vec_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1683237477188,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "Lc0y21sYKck7"
   },
   "outputs": [],
   "source": [
    "fps = 30\n",
    "def make_frame(t):\n",
    "  t = int(t * fps)\n",
    "  # print(t)\n",
    "  return video_and_audio_arr[t][0]\n",
    "\n",
    "from moviepy.video.io.VideoFileClip import VideoClip\n",
    "myclip = VideoClip(make_frame, duration = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1683237477188,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "-iBAZyubIpWH",
    "outputId": "db5114d7-4cc8-49fe-bfef-fc46b45426ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video test.mp4.\n",
      "Moviepy - Writing video test.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "myclip.write_videofile('test.mp4', fps = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1434,
     "status": "ok",
     "timestamp": 1683237478619,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "v6Cf944EXQd_",
    "outputId": "73683f4e-7a36-4056-e2cc-0908fcbeca2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1179072, 2)\n"
     ]
    }
   ],
   "source": [
    "## concatonate the audio\n",
    "audio = video_and_audio_arr[0][1]\n",
    "for i in range(20*40):\n",
    "  next_aud = video_and_audio_arr[i+1][1]\n",
    "  audio = np.vstack((audio, next_aud))\n",
    "print(audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3285,
     "status": "ok",
     "timestamp": 1683237481902,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "OW5Cdk0EZapo",
    "outputId": "788fb668-a99c-4b2a-9092-65270d1e6e8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pydub in /global/u2/m/martel/.local/perlmutter/pytorch1.13.1/lib/python3.9/site-packages (0.25.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1683237481903,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "z1UTKIkuYTMm"
   },
   "outputs": [],
   "source": [
    "import pydub \n",
    "\n",
    "def write(f, sr, x, normalized=True):\n",
    "    \"\"\"numpy array to MP3\"\"\"\n",
    "    channels = 2 if (x.ndim == 2 and x.shape[1] == 2) else 1\n",
    "    if normalized:  # normalized array - each item should be a float in [-1, 1)\n",
    "        y = np.int16(x * 2 ** 15)\n",
    "    else:\n",
    "        y = np.int16(x)\n",
    "    song = pydub.AudioSegment(y.tobytes(), frame_rate=sr, sample_width=2, channels=channels)\n",
    "    song.export(f, format=\"mp3\", bitrate=\"320k\")\n",
    "\n",
    "# sr = (audio_array.shape[0]// resized_vid_arr.shape[0])*fps\n",
    "#write('test_audio_2.mp3', sr, audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q8jot63w2ZZ"
   },
   "source": [
    "*training loop from cs189 hw*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1683237484501,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "MkAHFi0HxVso"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1683237484501,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "E2TzvSjoxnJz"
   },
   "outputs": [],
   "source": [
    "class VideoAudioDataset(Dataset):\n",
    "    def __init__(self, video_frames, audio_frames, num_frames):\n",
    "        self.video_frames = torch.tensor(video_frames, dtype=torch.float32).permute(0,3,2,1) # Permute to (N, C, H, W)\n",
    "        self.aud_per_frame = audio_frames.shape[0]// (video_frames.shape[0])\n",
    "        clip_amount = audio_frames.shape[0] % self.aud_per_frame\n",
    "        self.audio_frames = torch.tensor(audio_frames[:-clip_amount], dtype=torch.float32).reshape(-1, self.aud_per_frame,audio_frames.shape[1]).permute(0,2,1) # Permute to (N, C, A)\n",
    "        print(self.audio_frames.shape)\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += 1\n",
    "        if idx < self.num_frames:\n",
    "          num_zeros_needed = self.num_frames - idx\n",
    "          vid_zeros = torch.zeros(num_zeros_needed, *self.video_frames[0].shape)\n",
    "          aud_zeros = torch.zeros(num_zeros_needed, *self.audio_frames[0].shape)\n",
    "          vid = torch.vstack((vid_zeros, self.video_frames[0:idx])).transpose(0,1)\n",
    "          aud = torch.vstack((aud_zeros, self.audio_frames[0:idx])).transpose(0,1).reshape(2,-1)\n",
    "          return (vid, aud)\n",
    "        #vid shape example torch.Size([32, 3, 10, 36, 64])\n",
    "        # aud shape example torch.Size([32, 2, 14710])\n",
    "        vid = self.video_frames[idx-self.num_frames:idx].transpose(0,1)\n",
    "        aud = self.audio_frames[idx-self.num_frames: (idx)].transpose(0,1).reshape(2,-1)\n",
    "        # print('idx = ', idx, ' aud.size = ', aud.shape)\n",
    "        return (vid, aud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBlme3Oac8xI"
   },
   "source": [
    "https://github.com/antecessor/Wavenet << source!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683237484501,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "UfWN3pHBlP5V"
   },
   "outputs": [],
   "source": [
    "class AudConvEmbedding(nn.Module):\n",
    "  def __init__(self, in_channels, output_dim):\n",
    "    super(AudConvEmbedding, self).__init__()\n",
    "    self.conv_layers = nn.Sequential(\n",
    "        nn.Conv1d(in_channels = 2, out_channels = 2, kernel_size = 5, stride = 2),\n",
    "        nn.LayerNorm(7353),\n",
    "        nn.Conv1d(in_channels = 2, out_channels = 2, kernel_size = 5, stride = 2),\n",
    "        nn.LayerNorm(3675),\n",
    "        nn.Conv1d(in_channels = 2, out_channels = 2, kernel_size = 5, stride = 2),\n",
    "        nn.LayerNorm(1836),\n",
    "        nn.Conv1d(in_channels = 2, out_channels = 2, kernel_size = 5, stride = 2),\n",
    "        nn.LayerNorm(916),\n",
    "        nn.Conv1d(in_channels = 2, out_channels = 2, kernel_size = 5, stride = 2),\n",
    "        nn.LayerNorm(456),\n",
    "        nn.Linear(456, output_dim)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.conv_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1683237538333,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "HQcJkvjArLHd"
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def scaled_dot_product(self, q, k, v, mask=None):\n",
    "        d_k = q.size()[-1]\n",
    "        attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "        attn_logits = attn_logits / np.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "        attention = F.softmax(attn_logits, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Separate Q, K, V from linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        #print(q.shape, k.shape, v.shape)\n",
    "        # Determine value outputs\n",
    "        values, attention = self.scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        #print(values.shape)\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1683237538333,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "QYSe-ZnNdr9Y"
   },
   "outputs": [],
   "source": [
    "class AddPosEncoding(nn.Module):\n",
    "  def __init__(self, d_model = 256, input_dropout = 0.1, timing_dropout = 0.1, max_len = 512):\n",
    "    super(AddPosEncoding, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.input_dropout = input_dropout\n",
    "    self.timing_dropout = timing_dropout\n",
    "    self.max_len = max_len\n",
    "\n",
    "    self.timing_table = nn.Parameter(torch.FloatTensor(max_len, d_model))\n",
    "    nn.init.normal_(self.timing_table)\n",
    "    self.input_dropout = nn.Dropout(input_dropout)\n",
    "    self.timing_dropout = nn.Dropout(self.timing_dropout)\n",
    "\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = self.input_dropout(x)\n",
    "    timing = self.timing_table[None, :x.shape[1], :]\n",
    "    timing = self.timing_dropout(timing)\n",
    "    return x + timing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1683237538935,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "0tYyj_yXc1mE"
   },
   "outputs": [],
   "source": [
    "class AudTransformer(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, hidden_dims = 256, seq_len = 100, dim_ff = 1024, n_layers = 6, n_head = 8, d_qkv = 64, dropout = 0.1):\n",
    "        super(AudTransformer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.dim_ff = dim_ff\n",
    "        self.n_layers = n_layers\n",
    "        self.n_head = n_head\n",
    "        self.d_qkv = d_qkv\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.mha_list = nn.ModuleList()\n",
    "        self.mha_norms = nn.ModuleList()\n",
    "        self.pff_list = nn.ModuleList()\n",
    "        self.pff_norms = nn.ModuleList()\n",
    "\n",
    "        self.output_norm = nn.LayerNorm(hidden_dims)\n",
    "        self.linear = nn.Linear(self.hidden_dims, 2)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.add_timing = AddPosEncoding(hidden_dims, max_len = seq_len)\n",
    "\n",
    "        self.embedding = AudConvEmbedding(2, self.hidden_dims)\n",
    "        self.embedding2 = nn.Linear(2, self.hidden_dims)\n",
    "\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "          self.mha_list.append(MultiheadAttention(self.hidden_dims, self.d_qkv, self.n_head, self.dropout))\n",
    "          self.mha_norms.append(nn.LayerNorm(hidden_dims))\n",
    "          self.pff_list.append(nn.Sequential(\n",
    "                              nn.Linear(hidden_dims, dim_ff),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(dim_ff, hidden_dims),\n",
    "                              nn.Dropout(self.dropout)\n",
    "                              ))\n",
    "          self.pff_norms.append(nn.LayerNorm(hidden_dims))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x).transpose(2,1)\n",
    "    x = self.embedding2(x)\n",
    "    for i in range(self.n_layers):\n",
    "      x_norm = self.mha_norms[i](x)\n",
    "      #print('x_norm.shape', x_norm.shape)\n",
    "      att_out = self.mha_list[i](x)\n",
    "      #print(att_out.shape)\n",
    "      x = x_norm + att_out\n",
    "      x_norm = self.pff_norms[i](x)\n",
    "      ff_out = self.pff_list[i](x_norm)\n",
    "      x = x_norm + ff_out\n",
    "\n",
    "    x = self.output_norm(x)\n",
    "    x = x + self.add_timing(x)\n",
    "    x = self.linear(x)\n",
    "    x = self.tanh(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683237539153,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "uHtt4x660lxE"
   },
   "outputs": [],
   "source": [
    "class VidToAudFusion(nn.Module):\n",
    "    def __init__(self, audio_dim, video_dims, audio_channels, vid_channels, dropout_prob=0.1, num_frames=10):\n",
    "        super(VidToAudFusion, self).__init__()\n",
    "        self.dim_proj = nn.Linear(video_dims[0] * video_dims[1], audio_dim)\n",
    "        self.audio_dim = audio_dim\n",
    "        self.video_dims = video_dims\n",
    "        self.vid_channels = vid_channels\n",
    "\n",
    "        if audio_channels == vid_channels:\n",
    "            self.channel_projection = nn.Identity()\n",
    "        else:\n",
    "            self.channel_projection = nn.Sequential(\n",
    "                nn.Conv2d(vid_channels, audio_channels, kernel_size=1, stride=1, bias=True),\n",
    "            )\n",
    "        self.channel_projection2 = nn.Sequential(\n",
    "                nn.Conv1d(num_frames, 2, kernel_size=1, stride=1, bias=True),\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "      #x = x.reshape(x.shape[0], x.shape[1], x.shape[2], -1)\n",
    "      x = torch.flatten(x, start_dim=3)\n",
    "      audio_proj = self.dim_proj(x)\n",
    "      #print(audio_proj.shape)\n",
    "      audio_proj = self.channel_projection(audio_proj).squeeze()\n",
    "      audio_proj = self.channel_projection2(audio_proj)\n",
    "\n",
    "      #print(audio_proj.shape)\n",
    "      return audio_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683237539153,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "zXf3OZlZyA8M"
   },
   "outputs": [],
   "source": [
    "class ConvResBlock3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.1, stride=1, kernel_size=3, padding=1):\n",
    "        super(ConvResBlock3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "        self.norm1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=True)\n",
    "        self.norm2 = nn.BatchNorm3d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        if in_channels == out_channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        else:\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride, bias=True),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        residual = self.skip_connection(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        out = out + residual\n",
    "\n",
    "        return out\n",
    "\n",
    "class ConvResBlock2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.1, stride=1, kernel_size=3, padding=1):\n",
    "        super(ConvResBlock2D, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=True)\n",
    "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        if in_channels == out_channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        else:\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=True),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        residual = self.skip_connection(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        out = out + residual\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683237539153,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "Ue7zYZltyIe4"
   },
   "outputs": [],
   "source": [
    "### combines video into audio input vector and then passes through wavenet\n",
    "\n",
    "class AttAudVideoNet(nn.Module):\n",
    "    def __init__(self,audio_input_shape, video_input_shape,in_channels=2,out_channels=2): ## NEED TO DEBUG THESE hyperperams 4.29\n",
    "        super().__init__()\n",
    "        self.transformer=AudTransformer(in_channels, out_channels, hidden_dims = 256, seq_len = 14709, dim_ff = 1024, n_layers = 6, n_head = 8, d_qkv = 64, dropout = 0.1) \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.vid_convs = nn.ModuleList([\n",
    "            ConvResBlock3D(3, 64),\n",
    "            ConvResBlock3D(64, 128),\n",
    "            ConvResBlock3D(128, 256)\n",
    "        ])\n",
    "        self.audio_input_shape = audio_input_shape\n",
    "        self.video_input_shape = video_input_shape\n",
    "        vid_dims_list = self.get_video_dims(video_input_shape)\n",
    "        self.vid_to_aud = VidToAudFusion(self.audio_input_shape[-1], vid_dims_list[2], 1, 256, num_frames=video_input_shape[2])\n",
    "        \n",
    "        self.output_lin = nn.Linear(256*in_channels, out_channels)\n",
    "\n",
    "    def get_video_dims(self, video_input_shape):\n",
    "        shape_list = []\n",
    "        test_data = torch.ones(video_input_shape)\n",
    "        out = self.vid_convs[0](test_data)\n",
    "        shape_list.append((out.shape[-2],out.shape[-1]))\n",
    "        for layer in self.vid_convs[1:]:\n",
    "            out = layer(out)\n",
    "            shape_list.append((out.shape[-2],out.shape[-1]))\n",
    "        return shape_list\n",
    "\n",
    "    def forward(self,vid, aud):\n",
    "        for i in range(len(self.vid_convs)):\n",
    "          vid = self.vid_convs[i](vid)\n",
    "        vid_to_aud = self.vid_to_aud(vid)\n",
    "        #print(vid_to_aud.shape, aud.shape)\n",
    "        aud = aud + vid_to_aud\n",
    "        aud=self.transformer(aud)\n",
    "        aud=aud.reshape(aud.shape[0], -1)\n",
    "        aud = self.output_lin(aud)\n",
    "        #print('output shape = ', aud.shape)\n",
    "        return self.tanh(aud)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683237539153,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "N8JUAOeGGFnG",
    "outputId": "28b9d3b9-3c55-43f1-b32a-85f540449425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18547, 64, 36, 3) 128196864\n",
      "54583452\n"
     ]
    }
   ],
   "source": [
    "print(resized_vid_arr.shape, resized_vid_arr.size)\n",
    "print(audio_array.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1683237539810,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "Bs_v_OsGl5gC",
    "outputId": "29520eaa-b87e-4f16-bffd-ca0d3ce666f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14842, 2, 1471])\n",
      "torch.Size([3710, 2, 1471])\n"
     ]
    }
   ],
   "source": [
    "train_data = VideoAudioDataset(resized_vid_arr[:int(len(resized_vid_arr)*0.8)], audio_array[:int(len(audio_array)*0.8)], num_frames = 10)\n",
    "valid_data = VideoAudioDataset(resized_vid_arr[int(len(resized_vid_arr)*0.8):], audio_array[int(len(audio_array)*0.8):], num_frames = 10)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1683237541414,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "3flVTs2Q0n2e",
    "outputId": "74a7d44f-010c-4644-b20f-ff5fdbcc8fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 10, 36, 64])\n",
      "torch.Size([32, 2, 14710])\n"
     ]
    }
   ],
   "source": [
    "for x in train_loader:\n",
    "  print(x[0].shape)\n",
    "  print(x[1].shape)\n",
    "  break\n",
    "  \n",
    "count = 0\n",
    "# for x in valid_loader:\n",
    "#   count += 1\n",
    "#   if count == 10:\n",
    "#     print(x[0].shape)\n",
    "#     print(x[1].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "73273624d3ec46539047695496cef11b",
      "bbada1e125ff429eae418bf609e3ea4c",
      "9f611faec9f24d7fb136eae0dcaf71cf",
      "9c643f04c1b741d1b57fa7b6b209c102",
      "fdbb2926241c4891885ef8a36ce7a130",
      "46105105d11e435dbc0db488c43fdf47",
      "675868afca70457aacba30a626a61b35",
      "cdfb1461201943b183377eefdb6772d6",
      "3b20e87dd300401b82d747d27ecbc039",
      "d226292202f1430cbdc602085458ad03",
      "acadf0fed9184df98a87485271518fdc",
      "bda87b78f0cd420bb99623ea2fa2759f",
      "66028dc242a24a6bbe5aae7d5aac4ee0",
      "b23a60ed465e45beb5cd4bad947d1270",
      "1599d35bd6844badaeedd1fef5d86877",
      "85c3ed4a97f2464e834d340cf7d43006",
      "a646b672a12442fab793b40b5f019547",
      "fad159c751df456285bc240fd603ff37",
      "919ddb8d374b4e92b57f750c0d019859",
      "1bf569533ba648e08eedf0b0a2a9cdf9",
      "347b450562614c51b1295c8bddd69ade",
      "2f60c766deac41baa3e771dc24800dd1"
     ]
    },
    "id": "cz9cYVhaa0vj",
    "outputId": "e081301b-d979-4824-de7a-0b4008a8aa27"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3517f491724f3bbe6a70214f3a04a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc38e42858b44a98d90376bea1c3b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0437,  0.0390],\n",
      "        [ 0.3422,  0.4494],\n",
      "        [-0.0486,  0.3073]], device='cuda:0')\n",
      "tensor([[-0.0593,  0.7045],\n",
      "        [-0.0569,  0.7036],\n",
      "        [-0.0571,  0.7035]], device='cuda:0')\n",
      "0.06504486501216888\n",
      "0.5638364553451538\n",
      "loss for step 0 : 0.0040033239344863785\n",
      "epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d663e23431aa47b5bc54476ddacade63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7145a3836b1e46ef9539b22fb5871687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0265, -0.1051],\n",
      "        [-0.3529,  0.1588],\n",
      "        [ 0.0110,  0.2145]], device='cuda:0')\n",
      "tensor([[-0.9972,  0.9995],\n",
      "        [-0.9972,  0.9995],\n",
      "        [-0.9972,  0.9995]], device='cuda:0')\n",
      "0.042823806405067444\n",
      "-0.7293232679367065\n",
      "loss for step 0 : 0.004849589041069798\n",
      "epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8fe21efe2d24641997f7182f259dfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9289ef115bb744b59d3e46b244c1e4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0913, -0.4423],\n",
      "        [ 0.4524,  0.8149],\n",
      "        [-0.2571,  0.1161]], device='cuda:0')\n",
      "tensor([[-0.9988,  0.9997],\n",
      "        [-0.9988,  0.9997],\n",
      "        [-0.9988,  0.9997]], device='cuda:0')\n",
      "0.13779820501804352\n",
      "1.0651264190673828\n",
      "loss for step 0 : 0.004939245124873907\n",
      "epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0438963e6b4026ab50a6457b44e4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4b5deac1c24e81bd3f1fc0e6e7989e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4227,  0.0237],\n",
      "        [ 0.1960,  0.2058],\n",
      "        [-0.0824,  0.0391]], device='cuda:0')\n",
      "tensor([[-0.9994,  0.9998],\n",
      "        [-0.9994,  0.9998],\n",
      "        [-0.9994,  0.9998]], device='cuda:0')\n",
      "-0.8957185745239258\n",
      "0.442969411611557\n",
      "loss for step 0 : 0.004920380684020727\n",
      "epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8222dc005843f49719f7fc4722de7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcceeabee354862863f7c278f4d723c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0448, 0.1189],\n",
      "        [0.0328, 0.0890],\n",
      "        [0.0880, 0.0443]], device='cuda:0')\n",
      "tensor([[-0.9996,  0.9998],\n",
      "        [-0.9996,  0.9998],\n",
      "        [-0.9996,  0.9998]], device='cuda:0')\n",
      "0.11029749363660812\n",
      "0.08105745166540146\n",
      "loss for step 0 : 0.004877583561060221\n",
      "epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0fde3b90ea847089b5ac6a30968070d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3858e9f78b1e44ee94472d66b97d2e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5718, -0.3102],\n",
      "        [-0.2530, -0.3878],\n",
      "        [ 0.1339,  0.1435]], device='cuda:0')\n",
      "tensor([[-0.9997,  0.9999],\n",
      "        [-0.9997,  0.9999],\n",
      "        [-0.9997,  0.9999]], device='cuda:0')\n",
      "-1.255440354347229\n",
      "-0.587243378162384\n",
      "loss for step 0 : 0.004833814372187075\n",
      "epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862c7639cc1849248f70bf9eef2e8978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5a3cf61fe3483281a3f6e4e599b895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2225,  0.4066],\n",
      "        [-0.0607, -0.0225],\n",
      "        [ 0.1416, -0.0264]], device='cuda:0')\n",
      "tensor([[-0.9998,  0.9999],\n",
      "        [-0.9998,  0.9999],\n",
      "        [-0.9998,  0.9999]], device='cuda:0')\n",
      "0.5248762965202332\n",
      "-0.13187392055988312\n",
      "loss for step 0 : 0.004821755931429241\n",
      "epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf97144b1ee4cee83e131376ed67dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9191aa70afad4804aaaf16d361e4634a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1403, -0.3446],\n",
      "        [-0.0375, -0.1382],\n",
      "        [-0.0352,  0.2175]], device='cuda:0')\n",
      "tensor([[-0.9998,  0.9999],\n",
      "        [-0.9998,  0.9999],\n",
      "        [-0.9998,  0.9999]], device='cuda:0')\n",
      "-0.34204164147377014\n",
      "-0.09730452299118042\n",
      "loss for step 0 : 0.004876591131292314\n",
      "epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfe7e8c36b24b038c24137b1c3098f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b1cf18e385424eab1e11adf6372349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3224,  0.2060],\n",
      "        [-0.1116,  0.0582],\n",
      "        [ 0.0964,  0.0821]], device='cuda:0')\n",
      "tensor([[-0.9999,  1.0000],\n",
      "        [-0.9999,  1.0000],\n",
      "        [-0.9999,  1.0000]], device='cuda:0')\n",
      "-0.6594303846359253\n",
      "-0.23002825677394867\n",
      "loss for step 0 : 0.004833988756265329\n",
      "epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b5d6d859ed4c1a81914ac7cfe74f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd1258e7e324cb6b554095cb2a9754d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1786,  0.2097],\n",
      "        [ 0.6534,  0.4017],\n",
      "        [ 0.1815, -0.1190]], device='cuda:0')\n",
      "tensor([[-0.9999,  1.0000],\n",
      "        [-0.9999,  1.0000],\n",
      "        [-0.9999,  1.0000]], device='cuda:0')\n",
      "0.4063793122768402\n",
      "1.4405517578125\n",
      "loss for step 0 : 0.004856622931749924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 39\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m%\u001b[39mglobalStep\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     41\u001b[0m    \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[1;32m     42\u001b[0m    \u001b[38;5;66;03m# print(output.detach().numpy())\u001b[39;00m\n\u001b[1;32m     43\u001b[0m    \u001b[38;5;66;03m# print(y_train.numpy())\u001b[39;00m\n\u001b[1;32m     44\u001b[0m    \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/1.13.1/lib/python3.9/site-packages/torch/optim/adamw.py:316\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    314\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    318\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# wavenet = WaveNet(in_channels=2,out_channels=2,kernel_size=2,stack_size=23,layer_size=6).cuda().train()\n",
    "audio_test_data = torch.ones((1,2,14709))\n",
    "vid_test_data = torch.ones((1,3,10,36,64))\n",
    "\n",
    "wavenet = AttAudVideoNet(audio_input_shape=audio_test_data.shape, video_input_shape=vid_test_data.shape,in_channels=2,out_channels=2).cuda().train()\n",
    "\n",
    "\n",
    "optimizer=torch.optim.AdamW(wavenet.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "lossFunction = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def calc_accuracy(Out,Y):\n",
    "    max_vals, max_indices = torch.max(Out,1)\n",
    "    train_acc = (max_indices == Y).sum().item()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "  \n",
    "epochs= 20\n",
    "globalStep=3000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, (vid_frames,aud_frames) in tqdm(enumerate(train_loader),desc=\"Training\"):\n",
    "         #vid_frames = vid_frames.cuda()\n",
    "         target = aud_frames[:,:,-1].cuda()\n",
    "         aud_frames = aud_frames[:,:,:-1].cuda()\n",
    "         vid_frames = vid_frames.cuda()\n",
    "         #print(target.shape)\n",
    "         #print(aud_frames.shape)\n",
    "         output = wavenet(vid_frames, aud_frames).squeeze()\n",
    "         #print(output[0].detach().cpu().numpy(), target[0].cpu().numpy())\n",
    "         #print(output.shape)\n",
    "         #print(output)\n",
    "         #print(output.dtype, target.dtype)\n",
    "\n",
    "         loss = lossFunction(output,target)\n",
    "         optimizer.zero_grad()\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         if step%globalStep==0:\n",
    "            # scheduler.step()\n",
    "            # print(output.detach().numpy())\n",
    "            # print(y_train.numpy())\n",
    "            with torch.no_grad():\n",
    "                accuracy=0\n",
    "                val_loss=0\n",
    "                for stepTest, (vid_frames,aud_frames) in tqdm(enumerate(valid_loader),desc=\"Validation\"):\n",
    "                    vid_frames = vid_frames.cuda()\n",
    "                    target = aud_frames[:,:,-1].cuda()\n",
    "                    aud_frames = aud_frames[:,:,:-1].cuda()\n",
    "                    output = wavenet(vid_frames, aud_frames).squeeze()\n",
    "                    if stepTest==0:\n",
    "                        print(target[:3])\n",
    "                        print(output[:3])\n",
    "                        print(lossFunction(output[0],target[0]).item())\n",
    "                        print(lossFunction(output[1],target[1]).item())\n",
    "                    #accuracy+=calc_accuracy(output,target)*100\n",
    "                    val_loss+= lossFunction(output,target).item()\n",
    "                    if stepTest>200:\n",
    "                        print(output)\n",
    "                        break\n",
    "            print(f\"loss for step {step} : {val_loss/stepTest}\")\n",
    "\n",
    "         \n",
    "    print(f\"epoch {epoch}\")\n",
    "\n",
    "    save_path = 'transformer_vid_model_5.5.pt'\n",
    "    torch.save(wavenet.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 268,
     "status": "aborted",
     "timestamp": 1683237494989,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "JvGcaaiR_1wg"
   },
   "outputs": [],
   "source": [
    "save_path = 'transformer_vid_model_5.5.pt'\n",
    "#torch.save(wavenet.state_dict(), save_path)\n",
    "load_path = 'transformer_vid_model_5.5.pt'\n",
    "wavenet.load_state_dict(torch.load(load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1683237494989,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "AV772MGPyhZa"
   },
   "outputs": [],
   "source": [
    "class VideoOnlyDataset(Dataset):\n",
    "    def __init__(self, video_frames, num_frames):\n",
    "        self.video_frames = torch.tensor(video_frames, dtype=torch.float32).permute(0,3,2,1) # Permute to (N, C, H, W)\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += 1\n",
    "        if idx < self.num_frames:\n",
    "          num_zeros_needed = self.num_frames - idx\n",
    "          vid_zeros = torch.zeros(num_zeros_needed, *self.video_frames[0].shape)\n",
    "          vid = torch.vstack((vid_zeros, self.video_frames[0:idx])).transpose(0,1)\n",
    "          return vid\n",
    "        #vid shape example torch.Size([32, 3, 10, 36, 64])\n",
    "        # aud shape example torch.Size([32, 2, 14710])\n",
    "        vid = self.video_frames[idx-self.num_frames:idx].transpose(0,1)\n",
    "        # print('idx = ', idx, ' aud.size = ', aud.shape)\n",
    "        return vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "hTH9qnTGtkYk"
   },
   "outputs": [],
   "source": [
    "### generate audio for video from model\n",
    "num_frames = 10\n",
    "\n",
    "start_index = int(len(resized_vid_arr)*0.8) + num_frames\n",
    "audio_list = []\n",
    "vid_test_shape = (1,3,10,36,64)\n",
    "zero_frame = torch.zeros((vid_test_shape[3], vid_test_shape[4]))\n",
    "\n",
    "aud_per_vid_frame = 1471\n",
    "audio_start_index = int(len(audio_array)*0.8)\n",
    "print(audio_start_index)\n",
    "aud_input_arr = torch.tensor(audio_array[audio_start_index:audio_start_index+(num_frames * aud_per_vid_frame) - 1], dtype=torch.float32).cuda().T.unsqueeze(0)\n",
    "print(aud_input_arr.shape)\n",
    "\n",
    "#aud_input_arr = torch.zeros((1,2,(num_frames * aud_per_vid_frame) - 1)).cuda()\n",
    "print(aud_input_arr.shape)\n",
    "\n",
    "input_vid = resized_vid_arr[start_index:]\n",
    "gen_data = VideoOnlyDataset(input_vid, num_frames = num_frames)\n",
    "gen_loader = DataLoader(gen_data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "#NOTE: GEN NEEDS TO BE FIXED TO ACCOUNT FOR MULTIPLE AUDIO RUNS PER FRAME\n",
    "wavenet.eval()\n",
    "for i,vid in enumerate(gen_loader):\n",
    "    vid = vid.cuda()\n",
    "    print(vid.shape)\n",
    "    for j in trange(aud_per_vid_frame):\n",
    "        with torch.no_grad():\n",
    "            audio_output = wavenet(vid.cuda(), aud_input_arr.cuda()).cpu()\n",
    "            print(audio_output)\n",
    "        if j > 10:\n",
    "            break\n",
    "        audio_list.append(audio_output.squeeze().cpu().numpy())\n",
    "        aud_input_arr = aud_input_arr[:, :, 1:]\n",
    "        aud_input_arr = torch.cat((aud_input_arr, audio_output.cuda().unsqueeze(2)), 2)\n",
    "\n",
    "np.array(audio_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "MOPq8NXlCUlm"
   },
   "outputs": [],
   "source": [
    "print(len(audio_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "gnU6a3aV5Pa4"
   },
   "outputs": [],
   "source": [
    "print(np.array(audio_list).shape)\n",
    "audio_np = np.array(audio_list).transpose(1,0)\n",
    "audio_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "dHmMWv_fVdSp"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "audio_cpu = audio_output.cpu()\n",
    "audio_np = audio_cpu.detach().numpy()\n",
    "audio_np.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test_transformer_model_out_5.6_2.raw'\n",
    "np.save(path, audio_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "3zx_uQAZPKek"
   },
   "outputs": [],
   "source": [
    "fps = 30\n",
    "sr = (audio_array.shape[0]// resized_vid_arr.shape[0])*fps\n",
    "write('test_transformer_model_out_5.6_1.mp3', sr, audio_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "N_4yY3uMELFz"
   },
   "outputs": [],
   "source": [
    "print(audio_np[:,0,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "NksJ6xZi_MxT"
   },
   "outputs": [],
   "source": [
    "for x in train_loader:\n",
    "  print(x[0].shape, x[1].shape)\n",
    "  model(x[0], x[1])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "FbU2k6l3_Wkf"
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "IJdhs8O_BwDo"
   },
   "outputs": [],
   "source": [
    "del train_data\n",
    "del train_loader\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "t7CpHFCaBwyC"
   },
   "outputs": [],
   "source": [
    "audio_test_data = torch.ones((1,2,10,1472))\n",
    "vid_test_data = torch.ones((1,3,10,36,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494990,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "oP_zgSdG6Hy6"
   },
   "outputs": [],
   "source": [
    "test_res_block = ConvResBlock3D(3,32)\n",
    "print(vid_test_data.shape)\n",
    "test_out = test_res_block(vid_test_data)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1683237494991,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "kOdyHr0268R1"
   },
   "outputs": [],
   "source": [
    "test_res_block2 = ConvResBlock2D(2,32)\n",
    "print(audio_test_data.shape)\n",
    "test_out = test_res_block2(audio_test_data)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1683237494991,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "4colob4F7eH2"
   },
   "outputs": [],
   "source": [
    "test_lin = nn.Linear(audio_test_data.shape[-1], vid_test_data.shape[-1] * vid_test_data.shape[-2])\n",
    "ttt = test_lin(audio_test_data)\n",
    "ttt = ttt.reshape(ttt.shape[0],ttt.shape[1],ttt.shape[2],vid_test_data.shape[-2],vid_test_data.shape[-1])\n",
    "ttt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1683237494991,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "nU73SZMp8VhT"
   },
   "outputs": [],
   "source": [
    "fusion_test = AudToVidFusion(audio_dim = audio_test_data.shape[-1], video_dims = (vid_test_data.shape[-2],vid_test_data.shape[-1]), audio_channels=2, vid_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1683237494991,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "fc4D5oBA85Ei"
   },
   "outputs": [],
   "source": [
    "(fusion_test(audio_test_data) + vid_test_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494991,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "oWO4cP6h-pTV"
   },
   "outputs": [],
   "source": [
    "fusion_test2 = VidToAudFusion(audio_dim = audio_test_data.shape[-1], video_dims = (vid_test_data.shape[-2],vid_test_data.shape[-1]), audio_channels=2, vid_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494991,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "46NBmfm0_053"
   },
   "outputs": [],
   "source": [
    "(fusion_test2(vid_test_data) + audio_test_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1683237494991,
     "user": {
      "displayName": "Jackson Wagner",
      "userId": "13678973550726092608"
     },
     "user_tz": 420
    },
    "id": "f62amSXe_6Ur"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1hLH14IPIjmhukYwuIVkWH0KAWopjnaIe",
     "timestamp": 1683231309344
    },
    {
     "file_id": "1--9ZUZLPg4PBYeBwTwaaUQqzzREoTy5p",
     "timestamp": 1682834828374
    },
    {
     "file_id": "1DzDk2SAXwqU8pvN5-_zvgopIuH-s9Y0R",
     "timestamp": 1682711665945
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "pytorch-1.13.1",
   "language": "python",
   "name": "pytorch-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1599d35bd6844badaeedd1fef5d86877": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_347b450562614c51b1295c8bddd69ade",
      "placeholder": "",
      "style": "IPY_MODEL_2f60c766deac41baa3e771dc24800dd1",
      "value": " 46/? [00:19&lt;00:00,  2.54it/s]"
     }
    },
    "1bf569533ba648e08eedf0b0a2a9cdf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2f60c766deac41baa3e771dc24800dd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "347b450562614c51b1295c8bddd69ade": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b20e87dd300401b82d747d27ecbc039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "46105105d11e435dbc0db488c43fdf47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66028dc242a24a6bbe5aae7d5aac4ee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a646b672a12442fab793b40b5f019547",
      "placeholder": "",
      "style": "IPY_MODEL_fad159c751df456285bc240fd603ff37",
      "value": "Validation: "
     }
    },
    "675868afca70457aacba30a626a61b35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73273624d3ec46539047695496cef11b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbada1e125ff429eae418bf609e3ea4c",
       "IPY_MODEL_9f611faec9f24d7fb136eae0dcaf71cf",
       "IPY_MODEL_9c643f04c1b741d1b57fa7b6b209c102"
      ],
      "layout": "IPY_MODEL_fdbb2926241c4891885ef8a36ce7a130"
     }
    },
    "85c3ed4a97f2464e834d340cf7d43006": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "919ddb8d374b4e92b57f750c0d019859": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9c643f04c1b741d1b57fa7b6b209c102": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d226292202f1430cbdc602085458ad03",
      "placeholder": "",
      "style": "IPY_MODEL_acadf0fed9184df98a87485271518fdc",
      "value": " 0/? [00:00&lt;?, ?it/s]"
     }
    },
    "9f611faec9f24d7fb136eae0dcaf71cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdfb1461201943b183377eefdb6772d6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3b20e87dd300401b82d747d27ecbc039",
      "value": 0
     }
    },
    "a646b672a12442fab793b40b5f019547": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acadf0fed9184df98a87485271518fdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b23a60ed465e45beb5cd4bad947d1270": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_919ddb8d374b4e92b57f750c0d019859",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1bf569533ba648e08eedf0b0a2a9cdf9",
      "value": 1
     }
    },
    "bbada1e125ff429eae418bf609e3ea4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46105105d11e435dbc0db488c43fdf47",
      "placeholder": "",
      "style": "IPY_MODEL_675868afca70457aacba30a626a61b35",
      "value": "Training: "
     }
    },
    "bda87b78f0cd420bb99623ea2fa2759f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_66028dc242a24a6bbe5aae7d5aac4ee0",
       "IPY_MODEL_b23a60ed465e45beb5cd4bad947d1270",
       "IPY_MODEL_1599d35bd6844badaeedd1fef5d86877"
      ],
      "layout": "IPY_MODEL_85c3ed4a97f2464e834d340cf7d43006"
     }
    },
    "cdfb1461201943b183377eefdb6772d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "d226292202f1430cbdc602085458ad03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fad159c751df456285bc240fd603ff37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fdbb2926241c4891885ef8a36ce7a130": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
